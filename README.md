# üîç Sistema de Recupera√ß√£o de Informa√ß√µes com LangChain + FAISS + Ollama

Este projeto √© um exemplo pr√°tico de como criar um sistema de **perguntas e respostas baseado em um PDF**, usando **embeddings**, **banco vetorial** e **LLM**.

> üìÑ O conte√∫do usado como base foi um PDF sobre Redes Neurais.

---

## üß† O que ele faz?

- L√™ um PDF e divide o conte√∫do em partes menores (chunks).
- Gera **embeddings vetoriais** para cada parte usando o modelo `mxbai-embed-large`.
- Armazena esses vetores no banco de dados vetorial **FAISS**.
- Usa um LLM (`llama3.2:latest`, via Ollama) para responder perguntas com base no conte√∫do mais relevante do documento.
- Implementa um pipeline com o **RetrievalQA** da LangChain, que:
  - Transforma a pergunta do usu√°rio em embeddings.
  - Busca os vetores mais similares no banco FAISS.
  - Entrega essas informa√ß√µes ao LLM gerar uma resposta contextualizada.

---

## üöÄ Como rodar

### 1. Clone o reposit√≥rio

```bash
git clone https://github.com/RickHub2002/Retrieval
```

### 3. Instale as depend√™ncias

```bash
pip install langchain langchain-community langchain-ollama faiss-cpu
```

> ‚ö†Ô∏è Certifique-se tamb√©m de ter o [Ollama](https://ollama.com/) instalado e rodando localmente, com o modelo `llama3.2` j√° baixado:

```bash
ollama run llama3.2
```

---

## üß™ Exemplos de perguntas

```python
query = "Qual o tema principal do texto?"
response = qa_chain.invoke(query)
print("Resposta:", response)
```

---

## üìå Observa√ß√µes

- √â poss√≠vel ajustar o tamanho dos chunks (`chunk_size`) para melhorar a granularidade da busca.
- O n√∫mero de resultados retornados (`k` no `retriever`) tamb√©m influencia a qualidade da resposta.
- O modelo `llama3.2` roda localmente via Ollama, ent√£o **n√£o depende de APIs externas**.